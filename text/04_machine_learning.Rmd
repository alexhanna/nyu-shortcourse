---
title: "Machine Learning: Text Classification and Topic Modeling"
author: Alex Hanna
date: January 21, 2016
output: html_document
---

[&laquo; Text Analysis Module](../text/README.md)

The `RTextTools` and `stm` packages are the primary workhorse package we`re going to use.  

You can see the documentation for these at [https://cran.r-project.org/web/packages/RTextTools/RTextTools.pdf](https://cran.r-project.org/web/packages/RTextTools/RTextTools.pdf) 

Load the `tm` package,  `dplyr` and `tidyr` for data munging, and `ggplot2` for visualization.

```{r message=FALSE}
library(RTextTools)
library(tm)
library(dplyr)
library(tidyr)

library(ggplot2)
```

## Supervised text classification

Load the 20 newsgroups dataset

```{r results="hide"}
data <- read.csv("http://ssc.wisc.edu/~ahanna/20_newsgroups.csv", stringsAsFactors = FALSE)
data <- data[-1]
data <- tbl_df(data)
```

Have a look at the shape of the data.
```{r results="hide"}
names(data)
head(data$text, 2)
nrow(data)
```

Sample 20% of the data.
```{r}
data <- sample_frac(data, size = 0.1)
```

Create a Document-Term Matrix and apply a number of preprocessing transformations. Many preprocessing transformations take place by default, such as removing punctuation, converting to lowercase, and stripping whitespace.

```{r results="hide"}
dtm <- create_matrix(data, language="english")
dtm
```

We can get a sense of what this matrix looks like. The first argument is the range of documents, while the second is the range of terms.

```{r results="hide"}
inspect(dtm[1:10, 1:5])
```

Create a break between the training and test samples and create a container which can be used with `RTextTools` models.

```{r}
training_break <- as.integer(0.9*nrow(data))
container <- create_container(dtm,t(data$target),trainSize=1:training_break, testSize=training_break:nrow(data),virgin=FALSE)
```

Now we can cross-validate our supervised learning models. We can see which ones are available with print_algorithms()

```{r results="hide"}
print_algorithms()
```

For this workshop, we are using support vector machines (SVM). For cross-validaiton, I am choosing 3 folds. Ideally, we would play around with multiple classifiers and find the one which works the best for the task.

```{r results = 'hide'}
cv.svm <- cross_validate(container, 3, algorithm = 'SVM', kernel = 'linear')
```

This is the mean accuracy score.

```{r}
cv.svm$meanAccuracy
```

Now we can train the model, apply the trained model to our test set, and create analytics. We can see the [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall), and F1-score of the classifier and summarize them with `summary`. We can also see the SVM label, the correct label, and the SVM probability which was assigned to it.

```{r results = 'hide'}
models    <- train_model(container, algorithms = c("SVM"))
results   <- classify_model(container, models)
analytics <- create_analytics(container, results)
analytics@algorithm_summary
head(analytics@document_summary[1:3])
summary(analytics@algorithm_summary)
```

As a last analytic, we can plot the distribution of SVM probabilities against if they were correct versus whether they were incorrect.

```{r results = 'hide'}
p <- ggplot(analytics@document_summary, aes(SVM_PROB, fill = factor(CONSENSUS_INCORRECT)))
p <- p + geom_histogram()
p <- p + scale_fill_manual(values = c("grey", "red"), labels = c("Correct", "Incorrect"))
p <- p + theme_bw() + theme(legend.title = element_blank())

p <- p + facet_wrap(~ MANUAL_CODE)
```

## Topic Modeling
